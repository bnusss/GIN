{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_kronnc.py\n",
    "# random del 15 个nodes  email.txt\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "from networkx.convert import from_dict_of_dicts\n",
    "from networkx.classes.graph import Graph\n",
    "from kronEM import *\n",
    "seed =1900\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "adj_address =\"/data/chenmy/voter/seed1051email128-adjmat.pickle\"\n",
    "with open(adj_address,'rb') as f:\n",
    "    objective_adj = pickle.load(f,encoding='latin1')\n",
    "objective_adj = np.array(objective_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss nodes  tensor([117,  82, 114,  11,  48,  37,  88,  90,  33,  75, 115,  14])\n",
      "miss_edge 2928\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Ground_truth_adj = torch.FloatTensor(objective_adj)\n",
    "sz = Ground_truth_adj.shape[0]\n",
    "k = int(np.log2(sz))\n",
    "remove_proportion = 0.1\n",
    "del_num = int(sz*remove_proportion)\n",
    "mask_un_obs,mask_obs,miss_idx = missing_label(sz,remove_proportion)\n",
    "G = Ground_truth_adj*mask_obs\n",
    "nG = G.data.numpy()\n",
    "H = G.clone()\n",
    "# print(2,(abs(H.data.numpy()-objective_adj)*mask_obs.data.numpy()).sum())\n",
    "init_z = Ground_truth_adj*mask_un_obs\n",
    "missing_edges = int(init_z.sum())\n",
    "print(\"miss nodes \",miss_idx)\n",
    "# initial partial z with fixed missing edges num\n",
    "z_ele_num = int(mask_un_obs.sum())\n",
    "print(\"miss_edge\",z_ele_num)\n",
    "z_element_choice = torch.randperm(z_ele_num)[:missing_edges]\n",
    "z_element = torch.nonzero(mask_un_obs)\n",
    "init_z_edges = torch.index_select(z_element,0,z_element_choice)\n",
    "H[init_z_edges[:,0],init_z_edges[:,1]] = 1\n",
    "# print(abs((H - Ground_truth_adj)*mask_obs).sum())\n",
    "# print(G,H,z_element_choice)\n",
    "# initial kronecker\n",
    "p0 = torch.FloatTensor([[0.9,0.7],[0.7, 0.3]])#torch.FloatTensor([[0.4408, 0.1770],[0.4951, 0.2585]])  # \n",
    "generator = kronecker_Generator(p0,k,2)\n",
    "Pk = generator.generator_adjacency()\n",
    "init_H = H.data.numpy()\n",
    "label_non_obs = mask_un_obs.data.numpy()\n",
    "init_Pk = Pk.detach().data.numpy()\n",
    "perm = np.arange(sz)\n",
    "print(abs((init_H- objective_adj)*mask_obs.data.numpy()).sum())\n",
    "init_label_non_obs= label_non_obs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective p tensor([[0.9000, 0.7000],\n",
      "        [0.5000, 0.3000]])\n",
      "base tensor(1761.8107)\n",
      "base2 1761.811\n"
     ]
    }
   ],
   "source": [
    "# toy train data  generator\n",
    "# 删除0.25比例的节点 origin: we randomly removed 25% of the nodes and the corresponding edges.\n",
    "k = 7\n",
    "sz = 2**k\n",
    "remove_proportion = 0.1\n",
    "del_num = int(sz*remove_proportion)\n",
    "\n",
    "orip = torch.FloatTensor([[0.9,0.7],[0.5, 0.3]]) # torch.FloatTensor([[0.1981, 0.6427],[0.9684, 0.4522]])  \n",
    "print(\"objective p\", orip)\n",
    "ground_adj = generator_adj(k,orip)\n",
    "\n",
    "Ground_truth_adj_before = (ground_adj>torch.rand((ground_adj.shape))).float()\n",
    "# sigma=Ground_truth_adj\n",
    "\n",
    "\n",
    "\n",
    "baseline = loss_func(Ground_truth_adj_before,ground_adj,1)\n",
    "print(\"base\",baseline)\n",
    "\n",
    "np_baseline = NLL(Ground_truth_adj_before.data.numpy(),ground_adj.data.numpy())\n",
    "print(\"base2\",np_baseline)\n",
    "\n",
    "# shuffle \n",
    "node_num=sz\n",
    "permu_m = torch.eye(node_num)\n",
    "permutetion_arrange = torch.randperm(node_num)\n",
    "permu_m_shuffle= torch.index_select(permu_m,0,permutetion_arrange) \n",
    "\n",
    "Ground_truth_adj = torch.mm(permu_m_shuffle,Ground_truth_adj_before).mm(permu_m_shuffle.t())\n",
    "\n",
    "mask_un_obs,mask_obs,miss_idx = missing_label(sz,remove_proportion)\n",
    "G = Ground_truth_adj*mask_obs\n",
    "init_z = Ground_truth_adj*mask_un_obs\n",
    "missing_edges = int(init_z.sum())\n",
    "\n",
    "# initial partial z with fixed missing edges num\n",
    "z_ele_num = int(mask_un_obs.sum())\n",
    "z_element_choice = torch.randperm(z_ele_num)[:missing_edges]\n",
    "z_element = torch.nonzero(mask_un_obs)\n",
    "init_z_edges = torch.index_select(z_element,0,z_element_choice)\n",
    "H = G.clone()\n",
    "H[init_z_edges[:,0],init_z_edges[:,1]] = 1\n",
    "# print(G,H,z_element_choice)\n",
    "# initial kronecker\n",
    "\n",
    "p0 = torch.rand([2,2])  # torch.FloatTensor([[0.4408, 0.1770],[0.4951, 0.2585]]) \n",
    "generator = kronecker_Generator(p0,k,2)\n",
    "Pk = generator.generator_adjacency()\n",
    "init_H = H.data.numpy()\n",
    "label_non_obs = mask_un_obs.data.numpy()\n",
    "init_Pk = Pk.detach().data.numpy()\n",
    "init_label_non_obs= label_non_obs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(shape, eps=1e-20):\n",
    "    u = torch.rand(shape)\n",
    "    gumbel = - np.log(- np.log(u + eps) + eps)\n",
    "    if use_cuda:\n",
    "        gumbel = gumbel.cuda()\n",
    "    return gumbel\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    # gumbel_sample 返回一个sample采样\n",
    "    y = logits + gumbel_sample(logits.size())\n",
    "    return torch.nn.functional.softmax(y/temperature, dim=1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "    logits: [batch_size, n_class] unnormalized log-probs\n",
    "    temperature: non-negative scalar\n",
    "    hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "    [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "    If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "    be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    \n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if hard:\n",
    "        k = logits.size()[-1]\n",
    "        y_hard = torch.max(y.data, 1)[1]\n",
    "        y = y_hard\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load kronEM.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import numpy as np\n",
    "\n",
    "def kronecker(A,B):\n",
    "    return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "class kronecker_Generator(nn.Module):\n",
    "    def __init__(self,p0,korder = 3,node_num = 2):\n",
    "        super(kronecker_Generator,self).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        # self.p = Parameter(torch.rand(node_num,node_num,requires_grad=True))\n",
    "        self.korder = korder\n",
    "        # print(self.p)\n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj\n",
    "def loss_func(sigma,Pk):\n",
    "    loss = -torch.sum((1-sigma)*torch.log(1-Pk)+sigma*torch.log(Pk))\n",
    "    return loss\n",
    "def metropolis_update_ratio(sigma_before,sigma_later,Pk):\n",
    "    '''\n",
    "    if memory is sufficinet, this one is much cleaner to execute\n",
    "    '''\n",
    "    Nll_before=(1-sigma_before)*np.log(1-Pk) +sigma_before*np.log(Pk)\n",
    "    Nll_later=(1-sigma_later)*np.log(1-Pk) +sigma_later*np.log(Pk)\n",
    "    ratio=np.exp(np.sum(Nll_later-Nll_before))\n",
    "    return ratio\n",
    "def SwapElement(sigma_before,i,j):\n",
    "    i_topology=sigma_before[i,:]\n",
    "    j_topology=sigma_before[j,:]\n",
    "    sigma_later=np.copy(sigma_before)\n",
    "    sigma_later[i,:]=j_topology\n",
    "    sigma_later[j,:]=i_topology\n",
    "    sigma_later[:,i]=sigma_before[:,j]\n",
    "    sigma_later[:,j]=sigma_before[:,i]\n",
    "    sigma_later[i,j]=sigma_before[i,j]\n",
    "    sigma_later[j,i]=sigma_before[j,i]\n",
    "    sigma_later[i,i]=sigma_before[j,j]\n",
    "    sigma_later[j,j]=sigma_before[i,i]\n",
    "    return sigma_later\n",
    "\n",
    "def SamplePermutation(Pk,sigma,u,n1_swap,n2_swap,label_non_obs,obj_adj):\n",
    "    sigma_later=SwapElement(sigma,n1_swap, n2_swap)\n",
    "    ratio=metropolis_update_ratio(sigma,sigma_later,Pk)\n",
    "    if u<ratio:\n",
    "        sigma=sigma_later\n",
    "        label_non_obs=SwapElement(label_non_obs,n1_swap, n2_swap)\n",
    "        obj_adj = SwapElement(obj_adj,n1_swap,n2_swap)\n",
    "#         print(ifswap)\n",
    "    return sigma,label_non_obs,obj_adj\n",
    "def SampleZ(H,Pk,label_non_obs,u):\n",
    "    mat_size=len(Pk)\n",
    "    edge_in_non_obs=(H>0)*label_non_obs  \n",
    "    edge_position=np.where(edge_in_non_obs)\n",
    "    edge_removed=np.random.randint(len(edge_position[0]))\n",
    "    px=Pk[edge_position[0][edge_removed],edge_position[1][edge_removed]]\n",
    "    non_edge_in_non_obs=(H<1)*label_non_obs\n",
    "    ##return \n",
    "    py_array=non_edge_in_non_obs*Pk\n",
    "    py_array=np.ravel(py_array)\n",
    "    py=np.random.choice(range(len(py_array)), size=1, p=py_array/np.sum(py_array))\n",
    "    ratio=(1-py_array[py])/(1-px)\n",
    "    if ratio<u:\n",
    "        H[py//mat_size,py%mat_size]=1\n",
    "        H[edge_position[0][edge_removed],edge_position[1][edge_removed]]=0\n",
    "        # print('accept')\n",
    "    return H\n",
    "\n",
    "def missing_label(sz,missing_percent):\n",
    "     # random sample\n",
    "    missing_num = int(sz*missing_percent)\n",
    "    idx = torch.randperm(sz)[:missing_num]\n",
    "    mask_un_obs = torch.zeros(sz,sz)\n",
    "    for i in idx:\n",
    "        mask_un_obs[i] = 1\n",
    "        mask_un_obs[:,i:i+1] = 1\n",
    "    mask_obs = 1- mask_un_obs\n",
    "    return mask_un_obs,mask_obs,idx\n",
    "# only sigma and kronecker pk are itered\n",
    "def E_step(sigma,N,warmup,Pk,label_non_obs,obj_adj):\n",
    "    u1=np.random.rand(N+warmup)\n",
    "    u2=np.random.rand(N+warmup)\n",
    "    sigma_hist=[]\n",
    "    Z_label=[]\n",
    "    Node_list=np.arange(len(Pk))\n",
    "    element_to_swap=np.random.choice(a=Node_list,size=(2,3*(N+warmup)))\n",
    "    mask=element_to_swap[1,:]!=element_to_swap[0,:]# it is pointless to swap the same element\n",
    "    n1_swap=element_to_swap[0,:][mask]\n",
    "    n2_swap=element_to_swap[1,:][mask]\n",
    "    \n",
    "    for i in range(warmup):\n",
    "        sigma,label_non_obs,obj_adj=SamplePermutation(Pk,sigma,u2[i],n1_swap[i],n2_swap[i],label_non_obs,obj_adj)  \n",
    "    for j in range(warmup):\n",
    "        sigma=SampleZ(sigma,Pk,label_non_obs,u1[i]) \n",
    "    for k in range(N):\n",
    "        sigma=SampleZ(sigma,Pk,label_non_obs,u1[i]) \n",
    "        sigma_hist.append(sigma)\n",
    "    Z_label.append(label_non_obs)\n",
    "    return sigma_hist,Z_label,obj_adj #（H+G）\n",
    "\n",
    "def M_step(epoch,sigma_train,p0,k,N):\n",
    "    losses = []\n",
    "    generator = kronecker_Generator(p0,k,2)\n",
    "    learning_rate = 1e-5#0.0000001\n",
    "    opt_net = optim.SGD(generator.parameters(),lr = learning_rate)\n",
    "    decayRate = 0.95\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt_net, gamma=decayRate) \n",
    "    for i in range(epoch):   \n",
    "        opt_net.zero_grad()\n",
    "        Pk = generator.generator_adjacency()\n",
    "        loss = loss_func(sigma_train,Pk,N)\n",
    "        loss2 = loss2_func(sigma_train.detach().numpy(),Pk.detach().numpy(),N)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        # print(str(i),loss.item(),loss2.item(),(loss2-loss).item())\n",
    "        opt_net.step()\n",
    "        scheduler.step()\n",
    "        for group in opt_net.param_groups:\n",
    "            for param in group[\"params\"]: \n",
    "              # print(\"before param\",param)\n",
    "              param.data.clamp_(0.0001,0.9999)\n",
    "              # print(\"after param\",param)\n",
    "    \n",
    "        for p in generator.parameters():\n",
    "            p0 = p.data  \n",
    "        generator = kronecker_Generator(p0,k,2)\n",
    "        Pk = generator.generator_adjacency()\n",
    "        Pk = Pk.detach().numpy() \n",
    "        # evaluation\n",
    "        #   nll infer  nll true\n",
    "    return np.mean(losses),Pk,p0\n",
    "\n",
    "def kronEM(iterstep,N,warmup,H,Pk,label_non_obs,epoch,p0,obj_adj,k):\n",
    "    emlosses = []\n",
    "    for i in range(iterstep):\n",
    "        print(\"start iterstep\",i)\n",
    "        sigma_hist,Z_label,obj_adj = E_step(H,N,warmup,Pk,label_non_obs,obj_adj)\n",
    "        print(\"E-step\")\n",
    "        H = sigma_hist[-1]\n",
    "        label_non_obs = Z_label[-1]\n",
    "        sigma_hist_train = np.array(sigma_hist)\n",
    "        sigma_train = torch.DoubleTensor(sigma_hist_train)\n",
    "        emloss,Pk,p0 = M_step(epoch,sigma_train,p0,k,N)\n",
    "        emlosses.append(emloss)\n",
    "        print(\"*************\\n EM loss is %f\"%emloss,\"p0 is \",p0)\n",
    "        \n",
    "        # evaluation\n",
    "        inferp_nll = NLL(H,Pk)\n",
    "        print(\"inferp_nll is %f \"%(inferp_nll))\n",
    "        obs_mask = (1-label_non_obs).astype(bool)\n",
    "        obs_auc = calauc(obj_adj,Pk,obs_mask)\n",
    "        non_obs_auc = calauc(obj_adj,Pk,label_non_obs.astype(bool))\n",
    "        #         print(\"perm is \", perm)\n",
    "        \n",
    "        label_obs = (1-label_non_obs)\n",
    "        obs_diff_edge = abs((H - obj_adj)*label_obs).sum()\n",
    "        unobs_diff_edge =  abs((H - obj_adj)*label_non_obs).sum()\n",
    "        all_diff_edge = abs((H - obj_adj)).sum()\n",
    "        # D_abs = torch.abs(orip-p0).mean()\n",
    "        print(\"infer p is \",p0,  \"obseved auc  is %f and non_obs auc is %f and obs_diff_edge is %f and un_obs_diff_edge %f and all_diff_edge is %f\"%(obs_auc,non_obs_auc,obs_diff_edge,unobs_diff_edge,all_diff_edge))\n",
    "    \n",
    "    return emlosses,H,Pk\n",
    "\n",
    "def NLL(sigma_true,pk):\n",
    "    Nll_before=(1-sigma_true)*np.log(1-pk) +sigma_true*np.log(pk)\n",
    "    return -np.sum(Nll_before)\n",
    "\n",
    "def loss2_func(sigma_train,Pk,N):\n",
    "    loss2 = 0\n",
    "    for i in range(N):\n",
    "        loss2+= NLL(sigma_train[i],Pk)\n",
    "    # print(loss2)\n",
    "    return loss2/N\n",
    "\n",
    "def loss_func(sigma,Pk,N):\n",
    "    loss = -torch.sum((1-sigma)*torch.log(1-Pk)+sigma*torch.log(Pk))\n",
    "    return loss/N\n",
    "\n",
    "def generator_adj(korder,p):\n",
    "    k = korder\n",
    "    p0 = p\n",
    "    adj = p\n",
    "    for i in range(k-1):\n",
    "        adj = kronecker(adj,p0)\n",
    "    return adj\n",
    "def calauc(H,Pk,mask):\n",
    "    fpr, tpr, thresholds = roc_curve(H[mask],Pk[mask])\n",
    "    Auc = auc(fpr, tpr)\n",
    "    return Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup=30\n",
    "N = 15\n",
    "iterstep = 100\n",
    "epoch = 50\n",
    "emlosses2,H,pk= kronEM(iterstep,N,warmup,init_H,init_Pk,label_non_obs,epoch,p0,objective_adj,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start iterstep 0\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2434.850030 p0 is  tensor([[0.4206, 0.6819],\n",
      "        [0.5978, 0.3032]])\n",
      "inferp_nll is 2432.945557 \n",
      "infer p is  tensor([[0.4206, 0.6819],\n",
      "        [0.5978, 0.3032]]) obseved auc  is 0.484402 and non_obs auc is 0.523732 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 198.000000 and all_diff_edge is 1416.000000\n",
      "start iterstep 1\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2336.690232 p0 is  tensor([[0.4334, 0.6890],\n",
      "        [0.6070, 0.3245]])\n",
      "inferp_nll is 2335.270996 \n",
      "infer p is  tensor([[0.4334, 0.6890],\n",
      "        [0.6070, 0.3245]]) obseved auc  is 0.490670 and non_obs auc is 0.534361 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 198.000000 and all_diff_edge is 1416.000000\n",
      "start iterstep 2\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2251.935303 p0 is  tensor([[0.4454, 0.6957],\n",
      "        [0.6153, 0.3417]])\n",
      "inferp_nll is 2250.886230 \n",
      "infer p is  tensor([[0.4454, 0.6957],\n",
      "        [0.6153, 0.3417]]) obseved auc  is 0.486320 and non_obs auc is 0.555070 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 198.000000 and all_diff_edge is 1416.000000\n",
      "start iterstep 3\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2186.923733 p0 is  tensor([[0.4567, 0.7016],\n",
      "        [0.6232, 0.3555]])\n",
      "inferp_nll is 2186.137939 \n",
      "infer p is  tensor([[0.4567, 0.7016],\n",
      "        [0.6232, 0.3555]]) obseved auc  is 0.479646 and non_obs auc is 0.557377 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 198.000000 and all_diff_edge is 1416.000000\n",
      "start iterstep 4\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2129.833539 p0 is  tensor([[0.4660, 0.7073],\n",
      "        [0.6306, 0.3673]])\n",
      "inferp_nll is 2129.241699 \n",
      "infer p is  tensor([[0.4660, 0.7073],\n",
      "        [0.6306, 0.3673]]) obseved auc  is 0.490810 and non_obs auc is 0.560347 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 5\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2098.649065 p0 is  tensor([[0.4746, 0.7123],\n",
      "        [0.6372, 0.3773]])\n",
      "inferp_nll is 2098.191895 \n",
      "infer p is  tensor([[0.4746, 0.7123],\n",
      "        [0.6372, 0.3773]]) obseved auc  is 0.492401 and non_obs auc is 0.556731 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 6\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2072.062247 p0 is  tensor([[0.4826, 0.7170],\n",
      "        [0.6426, 0.3857]])\n",
      "inferp_nll is 2071.712402 \n",
      "infer p is  tensor([[0.4826, 0.7170],\n",
      "        [0.6426, 0.3857]]) obseved auc  is 0.504582 and non_obs auc is 0.551576 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 7\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2054.014987 p0 is  tensor([[0.4896, 0.7212],\n",
      "        [0.6472, 0.3931]])\n",
      "inferp_nll is 2053.747070 \n",
      "infer p is  tensor([[0.4896, 0.7212],\n",
      "        [0.6472, 0.3931]]) obseved auc  is 0.496719 and non_obs auc is 0.551755 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 8\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2040.374819 p0 is  tensor([[0.4964, 0.7248],\n",
      "        [0.6510, 0.3991]])\n",
      "inferp_nll is 2040.169189 \n",
      "infer p is  tensor([[0.4964, 0.7248],\n",
      "        [0.6510, 0.3991]]) obseved auc  is 0.495393 and non_obs auc is 0.560576 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 9\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2027.682558 p0 is  tensor([[0.5022, 0.7281],\n",
      "        [0.6542, 0.4044]])\n",
      "inferp_nll is 2027.528320 \n",
      "infer p is  tensor([[0.5022, 0.7281],\n",
      "        [0.6542, 0.4044]]) obseved auc  is 0.524488 and non_obs auc is 0.538327 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 10\n",
      "E-step\n",
      "*************\n",
      " EM loss is 2008.560206 p0 is  tensor([[0.5079, 0.7310],\n",
      "        [0.6568, 0.4081]])\n",
      "inferp_nll is 2008.444824 \n",
      "infer p is  tensor([[0.5079, 0.7310],\n",
      "        [0.6568, 0.4081]]) obseved auc  is 0.540912 and non_obs auc is 0.547614 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 11\n",
      "E-step\n",
      "*************\n",
      " EM loss is 1999.840894 p0 is  tensor([[0.5131, 0.7334],\n",
      "        [0.6592, 0.4109]])\n",
      "inferp_nll is 1999.753174 \n",
      "infer p is  tensor([[0.5131, 0.7334],\n",
      "        [0.6592, 0.4109]]) obseved auc  is 0.537822 and non_obs auc is 0.536552 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 12\n",
      "E-step\n",
      "*************\n",
      " EM loss is 1992.436922 p0 is  tensor([[0.5182, 0.7355],\n",
      "        [0.6611, 0.4128]])\n",
      "inferp_nll is 1992.366699 \n",
      "infer p is  tensor([[0.5182, 0.7355],\n",
      "        [0.6611, 0.4128]]) obseved auc  is 0.527603 and non_obs auc is 0.523910 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 13\n",
      "E-step\n",
      "*************\n",
      " EM loss is 1992.117880 p0 is  tensor([[0.5228, 0.7370],\n",
      "        [0.6628, 0.4144]])\n",
      "inferp_nll is 1992.062744 \n",
      "infer p is  tensor([[0.5228, 0.7370],\n",
      "        [0.6628, 0.4144]]) obseved auc  is 0.536021 and non_obs auc is 0.525458 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 14\n",
      "E-step\n",
      "*************\n",
      " EM loss is 1989.180587 p0 is  tensor([[0.5271, 0.7382],\n",
      "        [0.6641, 0.4156]])\n",
      "inferp_nll is 1989.137085 \n",
      "infer p is  tensor([[0.5271, 0.7382],\n",
      "        [0.6641, 0.4156]]) obseved auc  is 0.542590 and non_obs auc is 0.522164 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 15\n",
      "E-step\n",
      "*************\n",
      " EM loss is 1987.636388 p0 is  tensor([[0.5310, 0.7391],\n",
      "        [0.6653, 0.4165]])\n",
      "inferp_nll is 1987.601562 \n",
      "infer p is  tensor([[0.5310, 0.7391],\n",
      "        [0.6653, 0.4165]]) obseved auc  is 0.528062 and non_obs auc is 0.553982 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 16\n",
      "E-step\n",
      "*************\n",
      " EM loss is 1982.858687 p0 is  tensor([[0.5344, 0.7399],\n",
      "        [0.6663, 0.4172]])\n",
      "inferp_nll is 1982.832764 \n",
      "infer p is  tensor([[0.5344, 0.7399],\n",
      "        [0.6663, 0.4172]]) obseved auc  is 0.535561 and non_obs auc is 0.542326 and obs_diff_edge is 1218.000000 and un_obs_diff_edge 200.000000 and all_diff_edge is 1418.000000\n",
      "start iterstep 17\n",
      "E-step\n"
     ]
    }
   ],
   "source": [
    "warmup=30\n",
    "N = 15\n",
    "iterstep = 100\n",
    "epoch = 50\n",
    "emlosses2,H,pk= kronEM(iterstep,N,warmup,init_H,init_Pk,label_non_obs,epoch,p0,objective_adj,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_para =  label_non_obs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker(A,B):\n",
    "    return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "class kronecker_Generator(nn.Module):\n",
    "    def __init__(self,p0,korder = 3,node_num = 2):\n",
    "        super(kronecker_Generator,self).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        # self.p = Parameter(torch.rand(node_num,node_num,requires_grad=True))\n",
    "        self.korder = korder\n",
    "        # print(self.p)\n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gumbel_union_kron(nn.Module):\n",
    "    def __init__(self,p0,korder,perm,label_non_obs,temp=10,temp_drop_frac = 0.9999):\n",
    "        super(Gumbel_union_kron).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        self.korder = korder\n",
    "        self.perm = perm\n",
    "        self.label_non_obs = label_non_obs\n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj\n",
    "    \n",
    "    def sample_all(self,hard=False):\n",
    "        \n",
    "        self.logp = self.gen_matrix\n",
    "        if use_cuda:\n",
    "            self.logp = self.gen_matrix.cuda()\n",
    "        \n",
    "        out = gumbel_softmax(self.logp, self.temperature, hard)\n",
    "        if hard:\n",
    "            hh = torch.zeros((self.del_num*(2*self.sz - self.del_num-1),2))\n",
    "            for i in range(out.size()[0]):\n",
    "                hh[i, out[i]] = 1\n",
    "            out = hh                    \n",
    "        out = out[:, 0]\n",
    "        if use_cuda:\n",
    "            out = out.cuda()\n",
    "            \n",
    "        matrix = torch.zeros(self.sz,self.sz).cuda()\n",
    "        left_mask = torch.ones(self.sz,self.sz)\n",
    "        left_mask[:-self.del_num,:-self.del_num] = 0\n",
    "        left_mask = left_mask - torch.diag(torch.diag(left_mask))\n",
    "        un_index = left_mask.nonzero()\n",
    "        matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "        out_matrix = matrix\n",
    "        # out_matrix = out[:, 0].view(self.gen_matrix.size()[0], self.gen_matrix.size()[0])\n",
    "        return out_matrix\n",
    "    def init(self, mean, var):\n",
    "        init.normal_(self.gen_matrix, mean=mean, std=var)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''在网络补全任务中生成未知部分结构'''\n",
    "class Gumbel_Generator_nc_asy(nn.Module):\n",
    "    def __init__(self, sz=10,del_num = 1,temp=10, temp_drop_frac=0.9999):\n",
    "        super(Gumbel_Generator_nc_asy, self).__init__()\n",
    "        self.sz = sz\n",
    "        self.del_num = del_num\n",
    "        self.gen_matrix = Parameter(torch.rand(del_num*(2*sz-del_num-1), 2)) #cmy get only unknown part parameter\n",
    "        self.temperature = temp\n",
    "        self.temp_drop_frac = temp_drop_frac\n",
    "\n",
    "    def drop_temp(self):\n",
    "        # 降温过程\n",
    "        self.temperature = self.temperature * self.temp_drop_frac\n",
    "\n",
    "    def sample_all(self, hard=False):\n",
    "        self.logp = self.gen_matrix\n",
    "        if use_cuda:\n",
    "            self.logp = self.gen_matrix.cuda()\n",
    "        \n",
    "        out = gumbel_softmax(self.logp, self.temperature, hard)\n",
    "        if hard:\n",
    "            hh = torch.zeros((self.del_num*(2*self.sz - self.del_num-1),2))\n",
    "            for i in range(out.size()[0]):\n",
    "                hh[i, out[i]] = 1\n",
    "            out = hh                    \n",
    "        out = out[:, 0]\n",
    "        if use_cuda:\n",
    "            out = out.cuda()\n",
    "            \n",
    "        matrix = torch.zeros(self.sz,self.sz).cuda()\n",
    "        left_mask = torch.ones(self.sz,self.sz)\n",
    "        left_mask[:-self.del_num,:-self.del_num] = 0\n",
    "        left_mask = left_mask - torch.diag(torch.diag(left_mask))\n",
    "        un_index = left_mask.nonzero()\n",
    "        matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "        out_matrix = matrix\n",
    "        # out_matrix = out[:, 0].view(self.gen_matrix.size()[0], self.gen_matrix.size()[0])\n",
    "        return out_matrix\n",
    "    def init(self, mean, var):\n",
    "        init.normal_(self.gen_matrix, mean=mean, std=var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## union code \n",
    "kronfit 已知部分 + gumbel   + 两个loss （ll+states loss）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(2,size = ([3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_mat = np.array([0,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  85,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  65,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        37,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  22,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_mat  = np.eye(H.shape[0])[perm]\n",
    "shuffle_H = np.dot(np.dot(permutation_mat,objective_adj),permutation_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_later1=SwapElement(init_H,37,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_later=SwapElement(sigma_later,22,85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(sigma_later1-H)*mask_obs.data.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(shuffle_H-H)*mask_obs.data.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch03",
   "language": "python",
   "name": "torch03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
