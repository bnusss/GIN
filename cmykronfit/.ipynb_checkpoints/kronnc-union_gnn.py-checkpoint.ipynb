{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_kronnc.py\n",
    "# random del 15 个nodes  email.txt\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "from networkx.convert import from_dict_of_dicts\n",
    "from networkx.classes.graph import Graph\n",
    "from kronEM import *\n",
    "seed =1900\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker(A,B):\n",
    "    return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "\n",
    "def swapnodes(adj_before,i,j):\n",
    "    sz = adj_before.shape[0]\n",
    "    l = list(range(sz))\n",
    "    l[i],l[j] = l[j],l[i]\n",
    "    perm = torch.eye(sz)[l]\n",
    "    if use_cuda:\n",
    "        perm = perm.cuda()\n",
    "    adj_after = torch.mm(torch.mm(perm,adj_before),perm.T)\n",
    "    return adj_after    \n",
    "def metropolis_update_ratio_Pk(Pk_before,Pk_later,A):\n",
    "    '''\n",
    "    if memory is sufficinet, this one is much cleaner to execute\n",
    "    '''\n",
    "    Nll_before=(1-A)*torch.log(1-Pk_before) +A*torch.log(Pk_before)\n",
    "    Nll_later=(1-A)*torch.log(1-Pk_later) +A*torch.log(Pk_later)\n",
    "    ratio=torch.exp(torch.sum(Nll_later-Nll_before))\n",
    "    return ratio\n",
    "\n",
    "def swap_pk(pk,pos1,pos2,A,u):\n",
    "    pk_later = swapnodes(pk,pos1,pos2)\n",
    "    ratio = metropolis_update_ratio_Pk(pk,pk_later,A)\n",
    "    if u<ratio:\n",
    "        pk=pk_later\n",
    "    return pk\n",
    "\n",
    "\n",
    "class Gumbel_union_kron(nn.Module):\n",
    "    def __init__(self,p0,korder,label_non_obs,init_H,warmup,temp=10,temp_drop_frac = 0.9999):\n",
    "        super(Gumbel_union_kron,self).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        self.korder = korder\n",
    "        self.label_non_obs = label_non_obs\n",
    "        self.init_H  = init_H\n",
    "        self.warmup = warmup\n",
    "    \n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj\n",
    "    \n",
    "    def shuffle_adj(self):\n",
    "        pk = self.generator_adjacency()\n",
    "        if use_cuda:\n",
    "            pk = pk.cuda()\n",
    "        warmup =self.warmup\n",
    "        u1= torch.rand(warmup)\n",
    "        Node_list=np.arange(len(pk))\n",
    "        element_to_swap=np.random.choice(a=Node_list,size=(2,3*(warmup)))\n",
    "        mask=element_to_swap[1,:]!=element_to_swap[0,:]# it is pointless to swap the same element\n",
    "        n1_swap=element_to_swap[0,:][mask]\n",
    "        n2_swap=element_to_swap[1,:][mask]\n",
    "        for i in range(warmup):\n",
    "            pk = swap_pk(pk,n1_swap[i],n2_swap[i],self.init_H,u1[i])\n",
    "        return pk\n",
    "    \n",
    "    def sample_all(self,hard = True):\n",
    "        pk = self.shuffle_adj()\n",
    "        un_index = torch.nonzero(label_non_obs)\n",
    "        un_pk = pk[(un_index[:,0],un_index[:,1])].unsqueeze(1)\n",
    "        logp = torch.cat((un_pk,1-un_pk),1)\n",
    "        if use_cuda:\n",
    "            logp = logp.cuda()\n",
    "        out = gumbel_softmax(self.logp,self.temperature,hard)\n",
    "        if hard:\n",
    "            hh = torch.zeros_like(logp)\n",
    "            for i in range(out.size()[0]):\n",
    "                hh[i,out[i]] = 1\n",
    "                out = hh\n",
    "        out = out[:,0]\n",
    "        matrix = torch.zeros_like(pk)\n",
    "        matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "        \n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all parameter  {'node_num': 128, 'seed': 135, 'dim': 2, 'hid': 64, 'epoch_num': 700, 'batch_size': 1024, 'lr_net': 0.004, 'lr_dyn': 0.001, 'lr_state': 0.1, 'miss_percent': 0.1, 'data_path': '/data/chenmy/voter/seed2050email128128500', 'temp': 1, 'drop_frac': 1, 'cuda': 3}\n",
      "tensor([1., 3., 0., 3., 0., 0., 1., 3., 0., 0., 0., 3.], device='cuda:3')\n",
      "start_time: 2020-12-22 22:44:08\n",
      "data_num 357 object_matrix tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as U\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import sys \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from model_gy import *\n",
    "from tools_changedelmethod import *\n",
    "import os\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description = \"ER network\")\n",
    "parser.add_argument('--node_num', type=int, default=128,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument(\"--seed\",type =int,default = 135,help = \"random seed (default: 2050)\")\n",
    "# parser.add_argument(\"--sysdyn\",type = str,default = 'voter',help = \"the type of dynamics\")\n",
    "parser.add_argument(\"--dim\",type = int,default = 2,help = \"information diminsion of each node cml\")\n",
    "parser.add_argument(\"--hidden_size\",type = int,default = 64,help = \"hidden size of GGN model (default:128)\")\n",
    "parser.add_argument(\"--epoch_num\",type = int,default = 700,help = \"train epoch of model (default:1000)\")                    \n",
    "parser.add_argument(\"--batch_size\",type = int,default = 1024,help = \"input batch size for training (default: 128)\")\n",
    "parser.add_argument(\"--cuda\",type = int, default = 3,help = \"choose the GPU (default: 0)\")\n",
    "parser.add_argument(\"--lr_net\",type = float,default = 0.004,help = \"gumbel generator learning rate (default:0.004) \")\n",
    "parser.add_argument(\"--lr_dyn\",type = float,default = 0.001,help = \"dynamic learning rate (default:0.001)\")\n",
    "parser.add_argument(\"--lr_state\",type = float,default = 0.1,help = \"state learning rate (default:0.1)\")\n",
    "parser.add_argument(\"--miss_percent\",type = float,default = 0.1,help = \"missing percent node (default:0.1)\")\n",
    "parser.add_argument(\"--data_path\",type =str,default = '/data/chenmy/voter/seed2050email128128500',help = \"the path of simulation data (default:ER_p0.04100300) \")\n",
    "args = parser.parse_args([])\n",
    "# from model_old_generator import *\n",
    "# configuration\n",
    "HYP = {\n",
    "    'node_num': args.node_num,  # node num\n",
    "    'seed': args.seed,  # the seed\n",
    "    'dim': args.dim,  # information diminsion of each node cml:1 spring:4\n",
    "    'hid': args.hidden_size,  # hidden size\n",
    "    'epoch_num': args.epoch_num,  # epoch\n",
    "    'batch_size': args.batch_size,  # batch size\n",
    "    'lr_net':args.lr_net,  # lr for net generator\n",
    "    'lr_dyn': args.lr_dyn,  # lr for dyn learner\n",
    "    'lr_state':args.lr_state,\n",
    "    'miss_percent':args.miss_percent,\n",
    "    'data_path':args.data_path,\n",
    "    'temp': 1,  # 温度\n",
    "    'drop_frac': 1,  # temperature drop frac\n",
    "    \"cuda\":args.cuda,\n",
    "}\n",
    "\n",
    "print(\"all parameter \",HYP)\n",
    "# partial known adj \n",
    "torch.cuda.set_device(HYP[\"cuda\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(HYP['seed'])\n",
    "random.seed(HYP['seed'])\n",
    "del_num = int(args.node_num*args.miss_percent)\n",
    "kn_nodes = HYP['node_num'] - del_num\n",
    "# filename = os.path.abspath(__file__)\n",
    "# print(\"filename\",filename)\n",
    "\n",
    "\n",
    "num = 3\n",
    "def onehot_state(x_un_pre):\n",
    "    if x_un_pre.shape[1]==1:\n",
    "        state = torch.argmax(x_un_pre,2)\n",
    "        pre_state = torch.cat((state,1-state),1).unsqueeze(1)\n",
    "    else:\n",
    "        state = torch.argmax(x_un_pre,2).unsqueeze(2)\n",
    "        pre_state = torch.cat((state,1-state),2)  \n",
    "    return pre_state\n",
    "\n",
    "# load data\n",
    "adj_address = HYP['data_path']+'-adjmat.pickle'\n",
    "series_address = HYP['data_path']+'-series.pickle'\n",
    "# train_loader, val_loader, test_loader, object_matrix = load_bn_ggn_ori(series_address,adj_address,batch_size=HYP['batch_size'])\n",
    "train_loader, val_loader, test_loader, object_matrix = load_bn_ggn(series_address,adj_address,batch_size=HYP['batch_size'],seed = HYP['seed'])\n",
    "print(object_matrix[-del_num:,-del_num:,].sum(0))\n",
    "\n",
    "# check  未知与未知部分的link \n",
    "unedges  = torch.sum(object_matrix[-del_num:,-del_num:])\n",
    "while unedges.item() == 0:\n",
    "    print(\"sample 0 edges\")\n",
    "    sys.exit()\n",
    "\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print('start_time:', start_time)\n",
    "    \n",
    "# dyn learner isomorphism\n",
    "dyn_isom = IO_B_discrete(HYP['dim'], HYP['hid']).to(device)\n",
    "op_dyn = optim.Adam(dyn_isom.parameters(), lr=HYP['lr_dyn'])\n",
    "\n",
    "\n",
    "\n",
    "# states learner\n",
    "sample_num = len(train_loader.dataset)\n",
    "states_learner = Generator_states_discrete(sample_num,del_num).double()\n",
    "if use_cuda:\n",
    "    states_learner = states_learner.cuda()\n",
    "opt_states = optim.Adam(states_learner.parameters(),lr = HYP['lr_state'])\n",
    "train_index = DataLoader([i for i in range(sample_num)],HYP['batch_size'])\n",
    "\n",
    "# val  states learner \n",
    "v_sample_num = len(val_loader.dataset)\n",
    "states_learner_v = Generator_states_discrete(v_sample_num,del_num).double()\n",
    "if use_cuda:\n",
    "    states_learner_v = states_learner_v.cuda()\n",
    "opt_states_v = optim.Adam(states_learner_v.parameters(),lr = HYP['lr_state'])\n",
    "val_index = DataLoader([i for i in range(v_sample_num)],HYP['batch_size'])\n",
    "\n",
    "observed_adj = object_matrix[:-del_num,:-del_num]\n",
    "kn_mask,left_mask,un_un_mask,kn_un_mask = partial_mask(HYP['node_num'],del_num)\n",
    "print(\"data_num\",len(train_loader.dataset),\"object_matrix\",object_matrix)\n",
    "loss_fn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gener_init_H(obs,obj,label_non_obs):\n",
    "    missing_edges = int(torch.sum(obj*label_non_obs).item())\n",
    "    z_ele_num = int(torch.sum(label_non_obs).item())\n",
    "    z_element_choice = torch.randperm(z_ele_num)[:missing_edges]\n",
    "    if use_cuda:\n",
    "        z_element_choice = z_element_choice.cuda()\n",
    "    z_element = torch.nonzero(label_non_obs)\n",
    "#     print(z_element,z_element_choice)\n",
    "    init_z_edges = torch.index_select(z_element,0,z_element_choice)\n",
    "\n",
    "    obs[init_z_edges[:,0],init_z_edges[:,1]] = 1\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = gener_init_H(object_matrix*kn_mask.cuda(),object_matrix,left_mask.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = torch.FloatTensor([[0.8, 0.5],[0.4, 0.1]]) \n",
    "generator = Gumbel_union_kron(p0,korder=7,label_non_obs = left_mask,init_H = H,warmup = 30)\n",
    "op_net = optim.Adam(generator.parameters(), lr=HYP['lr_net'])\n",
    "# generator.sample_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练全网的正向动力学\n",
    "'''voter states,dyn,NET 同时训练 '''\n",
    "# indices to draw the curve\n",
    "num = 0\n",
    "def train_dyn_net_state():\n",
    "    loss_batch = []\n",
    "    ymae_batch = []\n",
    "    for idx,(data,states_id) in enumerate(zip(train_loader,train_index)):\n",
    "        x = data[0].float().to(device)\n",
    "        y = data[1].to(device).long()\n",
    "\n",
    "        x_kn = x[:,:-del_num,:]\n",
    "        y_kn = y[:,:-del_num]\n",
    "    \n",
    "        x_un = x[:,-del_num:,:]\n",
    "        generator.drop_temp()\n",
    "        outputs = torch.zeros(y.size(0), y.size(1),2)\n",
    "        loss_node = []\n",
    "        ymae_node = []\n",
    "        \n",
    "        for j in range(HYP['node_num']-del_num):\n",
    "            op_net.zero_grad()\n",
    "            op_dyn.zero_grad()\n",
    "            opt_states.zero_grad()\n",
    "            \n",
    "            x_un_pre = states_learner(states_id.cuda())\n",
    "            \n",
    "            x_hypo = torch.cat((x_kn,x_un_pre.float()),1)\n",
    "    \n",
    "            adj_col = generator.sample_all()[:,j]\n",
    "            adj_col[:-del_num] = observed_adj[j].cuda()\n",
    "\n",
    "            \n",
    "            y_hat = dyn_isom(x_hypo, adj_col, j, num, HYP['node_num']-del_num)\n",
    "            loss = loss_fn(y_hat,y[:,j])\n",
    "            loss.backward()\n",
    "\n",
    "            mae = torch.mean(abs(y[:,j] - torch.argmax(y_hat,1)).float())  \n",
    "\n",
    "            # cut gradient in case nan shows up\n",
    "            U.clip_grad_norm_(generator.gen_matrix, 0.000075)\n",
    "\n",
    "            op_dyn.step()\n",
    "            op_net.step()\n",
    "            opt_states.step()\n",
    "\n",
    "            # use outputs to caculate mse\n",
    "            outputs[:, j, :] = y_hat\n",
    "            \n",
    "            # record\n",
    "            ymae_node.append(mae.item())\n",
    "            loss_node.append(loss.item())\n",
    "\n",
    "        ymae_batch.append(np.mean(ymae_node))\n",
    "        loss_batch.append(np.mean(loss_node))\n",
    "        \n",
    "    return np.mean(loss_batch),np.mean(ymae_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# train\n",
    "val_epoch =  100\n",
    "choose_num  = int(HYP['node_num']/3)\n",
    "losses = []\n",
    "val_loss_epoch = []\n",
    "metric_epoch_yt=[]\n",
    "\n",
    "for epoch in range(HYP['epoch_num']):\n",
    "    start_time = time.time()\n",
    "    loss_y,mse_y = train_dyn_net_state()\n",
    "    (index_order,auc_net,precision,kn_un_precision,un_un_precision) = part_constructor_evaluator_sgm(generator,1,object_matrix,HYP[\"node_num\"],del_num)\n",
    "    metric_epoch_yt.append([auc_net,precision,kn_un_precision,un_un_precision])\n",
    "    losses.append([loss_y,mse_y])\n",
    "    # if auc_net[0]>0.95:\n",
    "    #     break\n",
    "    print(epoch,'gumbel all Net error: %f,kn_un :%f,un_un:%f'%(round(float(precision[1].item()/left_mask.sum()),2),round(float(kn_un_precision[1].item()/kn_un_mask.sum()),2),round(float(un_un_precision[1].item()/un_un_mask.sum()),2)))   \n",
    "    print(\"index_order\",index_order)\n",
    "    print('loss_y:%f,mse_y:%f'%(loss_y,mse_y))\n",
    "    end_time = time.time()\n",
    "    print(\"cost_time\",str(round(end_time -start_time, 2)))\n",
    "\n",
    "    if (epoch+1)%100 ==0:\n",
    "        print(\"\\nstatred val\",epoch)\n",
    "        val_losses = [];val_maes = []\n",
    "        for i in range(val_epoch):\n",
    "            val_loss,val_mae = val()\n",
    "            val_losses.append(val_loss)\n",
    "            val_maes.append(val_mae)\n",
    "        vloss = torch.mean(torch.FloatTensor(val_losses))\n",
    "        vmae = torch.mean(torch.FloatTensor(val_maes))\n",
    "        val_loss_epoch.append([vloss,vmae])\n",
    "        print('     val loss:' + str(vloss)+' val mse:' + str(vmae),'\\n')#\n",
    "end_time = time.time()\n",
    "print(\"cost_time\",str(round(end_time -start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    loss_batch = []\n",
    "    ymae_batch = []\n",
    "    choose_node = torch.randint(0,HYP['node_num']-del_num,[choose_num])\n",
    "    hypo_adj = generator.sample_all().detach()\n",
    "    hypo_adj[:-del_num,:-del_num] = observed_adj\n",
    "    for (idx,data),state_id in zip(enumerate(val_loader),val_index):\n",
    "        \n",
    "        x = data[0].float().to(device)\n",
    "        y = data[1].to(device).long()\n",
    "        \n",
    "        x_kn = x[:,:-del_num,:]\n",
    "        y_kn = y[:,:-del_num]\n",
    "    \n",
    "        x_un = x[:,-del_num:,:]\n",
    "        generator.drop_temp()\n",
    "        outputs = torch.zeros(y.size(0), y.size(1),2)\n",
    "        loss_node = []\n",
    "        states_node = []\n",
    "\n",
    "        for index,j in enumerate(choose_node):\n",
    "\n",
    "            x_un_pre = states_learner_v(state_id.cuda())\n",
    "            x = torch.cat((x_kn,x_un_pre.float()),1)\n",
    "            \n",
    "            opt_states_v.zero_grad()\n",
    "            adj_col = hypo_adj[:,j].cuda()# hard = true\n",
    "            y_hat = dyn_isom(x, adj_col, j, num, HYP['node_num'])\n",
    "            \n",
    "            loss = loss_fn(y_hat,y[:,j])\n",
    "            loss.backward()\n",
    "            opt_states_v.step()\n",
    "            ymae = torch.mean(abs(y[:,j] - torch.argmax(y_hat,1)).float())  \n",
    "            # states_node\n",
    "            outputs[:, index, :] = y_hat\n",
    "            \n",
    "            # record\n",
    "            loss_node.append(loss.item())            \n",
    "            states_node.append(ymae.item())\n",
    "\n",
    "        ymae_batch.append(np.mean(states_node))\n",
    "        loss_batch.append(np.mean(loss_node))\n",
    "        \n",
    "    return torch.mean(torch.FloatTensor(loss_batch)),torch.mean(torch.FloatTensor(ymae_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kronecker_Generator(nn.Module):\n",
    "    def __init__(self,p0,korder = 3,node_num = 2):\n",
    "        super(kronecker_Generator,self).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        # self.p = Parameter(torch.rand(node_num,node_num,requires_grad=True))\n",
    "        self.korder = korder\n",
    "        # print(self.p)\n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj\n",
    "def loss_func(sigma,Pk):\n",
    "    loss = -torch.sum((1-sigma)*torch.log(1-Pk)+sigma*torch.log(Pk))\n",
    "    return loss\n",
    "def metropolis_update_ratio(sigma_before,sigma_later,Pk):\n",
    "    '''\n",
    "    if memory is sufficinet, this one is much cleaner to execute\n",
    "    '''\n",
    "    Nll_before=(1-sigma_before)*np.log(1-Pk) +sigma_before*np.log(Pk)\n",
    "    Nll_later=(1-sigma_later)*np.log(1-Pk) +sigma_later*np.log(Pk)\n",
    "    ratio=np.exp(np.sum(Nll_later-Nll_before))\n",
    "    return ratio\n",
    "def SwapElement(sigma_before,i,j):\n",
    "    i_topology=sigma_before[i,:]\n",
    "    j_topology=sigma_before[j,:]\n",
    "    sigma_later=np.copy(sigma_before)\n",
    "    sigma_later[i,:]=j_topology\n",
    "    sigma_later[j,:]=i_topology\n",
    "    sigma_later[:,i]=sigma_before[:,j]\n",
    "    sigma_later[:,j]=sigma_before[:,i]\n",
    "    sigma_later[i,j]=sigma_before[i,j]\n",
    "    sigma_later[j,i]=sigma_before[j,i]\n",
    "    sigma_later[i,i]=sigma_before[j,j]\n",
    "    sigma_later[j,j]=sigma_before[i,i]\n",
    "    return sigma_later\n",
    "\n",
    "def SamplePermutation(Pk,sigma,u,n1_swap,n2_swap,label_non_obs,obj_adj):\n",
    "    sigma_later=SwapElement(sigma,n1_swap, n2_swap)\n",
    "    ratio=metropolis_update_ratio(sigma,sigma_later,Pk)\n",
    "    if u<ratio:\n",
    "        sigma=sigma_later\n",
    "        label_non_obs=SwapElement(label_non_obs,n1_swap, n2_swap)\n",
    "        obj_adj = SwapElement(obj_adj,n1_swap,n2_swap)\n",
    "#         print(ifswap)\n",
    "    return sigma,label_non_obs,obj_adj\n",
    "def SampleZ(H,Pk,label_non_obs,u):\n",
    "    mat_size=len(Pk)\n",
    "    edge_in_non_obs=(H>0)*label_non_obs  \n",
    "    edge_position=np.where(edge_in_non_obs)\n",
    "    edge_removed=np.random.randint(len(edge_position[0]))\n",
    "    px=Pk[edge_position[0][edge_removed],edge_position[1][edge_removed]]\n",
    "    non_edge_in_non_obs=(H<1)*label_non_obs\n",
    "    ##return \n",
    "    py_array=non_edge_in_non_obs*Pk\n",
    "    py_array=np.ravel(py_array)\n",
    "    py=np.random.choice(range(len(py_array)), size=1, p=py_array/np.sum(py_array))\n",
    "    ratio=(1-py_array[py])/(1-px)\n",
    "    if ratio<u:\n",
    "        H[py//mat_size,py%mat_size]=1\n",
    "        H[edge_position[0][edge_removed],edge_position[1][edge_removed]]=0\n",
    "        # print('accept')\n",
    "    return H\n",
    "\n",
    "def missing_label(sz,missing_percent):\n",
    "     # random sample\n",
    "    missing_num = int(sz*missing_percent)\n",
    "    idx = torch.randperm(sz)[:missing_num]\n",
    "    mask_un_obs = torch.zeros(sz,sz)\n",
    "    for i in idx:\n",
    "        mask_un_obs[i] = 1\n",
    "        mask_un_obs[:,i:i+1] = 1\n",
    "    mask_obs = 1- mask_un_obs\n",
    "    return mask_un_obs,mask_obs,idx\n",
    "# only sigma and kronecker pk are itered\n",
    "def E_step(sigma,N,warmup,Pk,label_non_obs,obj_adj):\n",
    "    u1=np.random.rand(N+2*warmup)\n",
    "    u2=np.random.rand(N+2*warmup)\n",
    "    sigma_hist=[]\n",
    "    Z_label=[]\n",
    "    Node_list=np.arange(len(Pk))\n",
    "    element_to_swap=np.random.choice(a=Node_list,size=(2,3*(N+warmup)))\n",
    "    mask=element_to_swap[1,:]!=element_to_swap[0,:]# it is pointless to swap the same element\n",
    "    n1_swap=element_to_swap[0,:][mask]\n",
    "    n2_swap=element_to_swap[1,:][mask]\n",
    "    \n",
    "    for i in range(warmup):\n",
    "        sigma,label_non_obs,obj_adj=SamplePermutation(Pk,sigma,u2[i],n1_swap[i],n2_swap[i],label_non_obs,obj_adj)  \n",
    "    for j in range(warmup):\n",
    "        sigma=SampleZ(sigma,Pk,label_non_obs,u1[i+j:i+j+1]) \n",
    "    for k in range(N):\n",
    "        sigma=SampleZ(sigma,Pk,label_non_obs,u1[i+j+1+k:i+j+1+k+1]) \n",
    "        sigma_hist.append(sigma)\n",
    "    Z_label.append(label_non_obs)\n",
    "    return sigma_hist,Z_label,obj_adj #（H+G）\n",
    "\n",
    "def M_step(epoch,sigma_train,p0,k,N):\n",
    "    losses = []\n",
    "    generator = kronecker_Generator(p0,k,2)\n",
    "    learning_rate = 1e-5#0.0000001\n",
    "    opt_net = optim.SGD(generator.parameters(),lr = learning_rate)\n",
    "    decayRate = 0.95\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt_net, gamma=decayRate) \n",
    "    for i in range(epoch):   \n",
    "        opt_net.zero_grad()\n",
    "        Pk = generator.generator_adjacency()\n",
    "        loss = loss_func(sigma_train,Pk,N)\n",
    "        loss2 = loss2_func(sigma_train.detach().numpy(),Pk.detach().numpy(),N)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        # print(str(i),loss.item(),loss2.item(),(loss2-loss).item())\n",
    "        opt_net.step()\n",
    "        scheduler.step()\n",
    "        for group in opt_net.param_groups:\n",
    "            for param in group[\"params\"]: \n",
    "              # print(\"before param\",param)\n",
    "              param.data.clamp_(0.0001,0.9999)\n",
    "              # print(\"after param\",param)\n",
    "    \n",
    "        for p in generator.parameters():\n",
    "            p0 = p.data  \n",
    "        generator = kronecker_Generator(p0,k,2)\n",
    "        Pk = generator.generator_adjacency()\n",
    "        Pk = Pk.detach().numpy() \n",
    "        # evaluation\n",
    "        #   nll infer  nll true\n",
    "    return np.mean(losses),Pk,p0\n",
    "\n",
    "def kronEM(iterstep,N,warmup,H,Pk,label_non_obs,epoch,p0,obj_adj,k):\n",
    "    emlosses = []\n",
    "    for i in range(iterstep):\n",
    "        print(\"start iterstep\",i)\n",
    "        sigma_hist,Z_label,obj_adj = E_step(H,N,warmup,Pk,label_non_obs,obj_adj)\n",
    "        print(\"E-step\")\n",
    "        H = sigma_hist[-1]\n",
    "        label_non_obs = Z_label[-1]\n",
    "        sigma_hist_train = np.array(sigma_hist)\n",
    "        sigma_train = torch.DoubleTensor(sigma_hist_train)\n",
    "        emloss,Pk,p0 = M_step(epoch,sigma_train,p0,k,N)\n",
    "        emlosses.append(emloss)\n",
    "        print(\"*************\\n EM loss is %f\"%emloss,\"p0 is \",p0)\n",
    "        \n",
    "        # evaluation\n",
    "        inferp_nll = NLL(H,Pk)\n",
    "        print(\"inferp_nll is %f \"%(inferp_nll))\n",
    "        obs_mask = (1-label_non_obs).astype(bool)\n",
    "        obs_auc = calauc(obj_adj,Pk,obs_mask)\n",
    "        non_obs_auc = calauc(obj_adj,Pk,label_non_obs.astype(bool))\n",
    "        #         print(\"perm is \", perm)\n",
    "        \n",
    "        label_obs = (1-label_non_obs)\n",
    "        obs_diff_edge = abs((H - obj_adj)*label_obs).sum()\n",
    "        unobs_diff_edge =  abs((H - obj_adj)*label_non_obs).sum()\n",
    "  \n",
    "        all_diff_edge = abs((H - obj_adj)).sum()\n",
    "        # D_abs = torch.abs(orip-p0).mean()\n",
    "        c=calauc(H,Pk,(label_non_obs).astype(bool))\n",
    "        print(c)\n",
    "        print(\"infer p is \",p0,  \"obseved auc  is %f and non_obs auc is %f and obs_diff_edge is %f and un_obs_diff_edge %f and all_diff_edge is %f\"%(obs_auc,non_obs_auc,obs_diff_edge,unobs_diff_edge,all_diff_edge))\n",
    "    \n",
    "    return emlosses,H,Pk,label_non_obs,obj_adj\n",
    "\n",
    "def NLL(sigma_true,pk):\n",
    "    Nll_before=(1-sigma_true)*np.log(1-pk) +sigma_true*np.log(pk)\n",
    "    return -np.sum(Nll_before)\n",
    "\n",
    "def loss2_func(sigma_train,Pk,N):\n",
    "    loss2 = 0\n",
    "    for i in range(N):\n",
    "        loss2+= NLL(sigma_train[i],Pk)\n",
    "    # print(loss2)\n",
    "    return loss2/N\n",
    "\n",
    "def loss_func(sigma,Pk,N):\n",
    "    loss = -torch.sum((1-sigma)*torch.log(1-Pk)+sigma*torch.log(Pk))\n",
    "    return loss/N\n",
    "\n",
    "def generator_adj(korder,p):\n",
    "    k = korder\n",
    "    p0 = p\n",
    "    adj = p\n",
    "    for i in range(k-1):\n",
    "        adj = kronecker(adj,p0)\n",
    "    return adj\n",
    "def calauc(H,Pk,mask):\n",
    "    fpr, tpr, thresholds = roc_curve(H[mask],Pk[mask])\n",
    "    Auc = auc(fpr, tpr)\n",
    "    return Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective p tensor([[0.9000, 0.6000],\n",
      "        [0.4000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "k =7\n",
    "sz = 2**k\n",
    "remove_proportion = 0.25\n",
    "del_num = int(sz*remove_proportion)\n",
    "\n",
    "orip = torch.FloatTensor([[0.9,0.6],[0.4, 0.2]]) # torch.FloatTensor([[0.1981, 0.6427],[0.9684, 0.4522]])  \n",
    "print(\"objective p\", orip)\n",
    "ground_adj = generator_adj(k,orip)\n",
    "\n",
    "Ground_truth_adj_before = (ground_adj>torch.rand((ground_adj.shape))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base tensor(791.8375)\n",
      "base2 791.83716\n"
     ]
    }
   ],
   "source": [
    "# sigma=Ground_truth_adj\n",
    "baseline = loss_func(Ground_truth_adj_before,ground_adj,1)\n",
    "print(\"base\",baseline)\n",
    "\n",
    "np_baseline = NLL(Ground_truth_adj_before.data.numpy(),ground_adj.data.numpy())\n",
    "print(\"base2\",np_baseline)\n",
    "\n",
    "# shuffle \n",
    "node_num=sz\n",
    "permu_m = torch.eye(node_num)\n",
    "permutetion_arrange = torch.randperm(node_num)\n",
    "permu_m_shuffle= torch.index_select(permu_m,0,permutetion_arrange) \n",
    "\n",
    "Ground_truth_adj = torch.mm(permu_m_shuffle,Ground_truth_adj_before).mm(permu_m_shuffle.t())\n",
    "Ground_truth_adj =Ground_truth_adj_before\n",
    "mask_un_obs,mask_obs,miss_idx = missing_label(sz,remove_proportion)\n",
    "G = Ground_truth_adj*mask_obs\n",
    "init_z = Ground_truth_adj*mask_un_obs\n",
    "missing_edges = int(init_z.sum())\n",
    "\n",
    "# initial partial z with fixed missing edges num\n",
    "z_ele_num = int(mask_un_obs.sum())\n",
    "z_element_choice = torch.randperm(z_ele_num)[:missing_edges]\n",
    "z_element = torch.nonzero(mask_un_obs)\n",
    "init_z_edges = torch.index_select(z_element,0,z_element_choice)\n",
    "H = G.clone()\n",
    "H[init_z_edges[:,0],init_z_edges[:,1]] = 1\n",
    "# print(G,H,z_element_choice)\n",
    "# initial kronecker\n",
    "\n",
    "p0 = torch.FloatTensor([[0.8, 0.5],[0.4, 0.1]])   # torch.rand([2,2])\n",
    "generator = kronecker_Generator(p0,k,2)\n",
    "Pk = generator.generator_adjacency()\n",
    "init_H = H.data.numpy()\n",
    "label_non_obs = mask_un_obs.data.numpy()\n",
    "init_Pk = Pk.detach().data.numpy()\n",
    "init_label_non_obs= label_non_obs.copy()\n",
    "objective_adj = np.array(Ground_truth_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SwapElement(sigma_before,i,j):\n",
    "    i_topology=sigma_before[i,:]\n",
    "    j_topology=sigma_before[j,:]\n",
    "    sigma_later=np.copy(sigma_before)\n",
    "    sigma_later[i,:]=j_topology\n",
    "    sigma_later[j,:]=i_topology\n",
    "    sigma_later[:,i]=sigma_before[:,j]\n",
    "    sigma_later[:,j]=sigma_before[:,i]\n",
    "    sigma_later[i,j]=sigma_before[i,j]\n",
    "    sigma_later[j,i]=sigma_before[j,i]\n",
    "    sigma_later[i,i]=sigma_before[j,j]\n",
    "    sigma_later[j,j]=sigma_before[i,i]\n",
    "    \n",
    "    return sigma_later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getswappk(warmup):\n",
    "    u1=np.random.rand(warmup)\n",
    "    Node_list=np.arange(len(Pk))\n",
    "    element_to_swap=np.random.choice(a=Node_list,size=(2,3*(warmup)))\n",
    "    mask=element_to_swap[1,:]!=element_to_swap[0,:]# it is pointless to swap the same element\n",
    "    n1_swap=element_to_swap[0,:][mask]\n",
    "    n2_swap=element_to_swap[1,:][mask]\n",
    "    for i in range(warmup):\n",
    "        pk = SamplePermutationPk(init_Pk,n1_swap[i],n2_swap[i],index,init_H,u1[i])\n",
    "    return pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9255, 0.6935, 0.1863, 0.6986, 0.5002, 0.5029, 0.6360, 0.4869, 0.2306,\n",
       "        0.7959, 0.0481, 0.2977, 0.9555, 0.3731, 0.7308, 0.7063, 0.9098, 0.8276,\n",
       "        0.6475, 0.0466, 0.5031, 0.4216, 0.4395, 0.4553, 0.8027, 0.4982, 0.1635,\n",
       "        0.7212, 0.7280, 0.0915, 0.3064, 0.8402, 0.0142, 0.3212, 0.1841, 0.7992,\n",
       "        0.7759, 0.9877, 0.0595, 0.4103, 0.5849, 0.0304, 0.3970, 0.1216, 0.7669,\n",
       "        0.8689, 0.4970, 0.0456, 0.3238, 0.5318, 0.6152, 0.3125, 0.8939, 0.8318,\n",
       "        0.4548, 0.1058, 0.7522, 0.5967, 0.1546, 0.7176, 0.4060, 0.2343, 0.7893,\n",
       "        0.7611, 0.1940, 0.8506, 0.7182, 0.0822, 0.2599, 0.0214, 0.8159, 0.0642,\n",
       "        0.6513, 0.0272, 0.2667, 0.5180, 0.7042, 0.4255, 0.0788, 0.2195, 0.1731,\n",
       "        0.8244, 0.6016, 0.7863, 0.3811, 0.7120, 0.6345, 0.5151, 0.6208, 0.6456,\n",
       "        0.5823, 0.6553, 0.3517, 0.1856, 0.6264, 0.9657, 0.0824, 0.3345, 0.2266,\n",
       "        0.4657])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sample_all(self, hard=False):\n",
    "        self.logp = self.gen_matrix\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.logp = self.gen_matrix.cuda()\n",
    "        \n",
    "        out = gumbel_softmax(self.logp, self.temperature, hard)\n",
    "        if hard:\n",
    "            hh = torch.zeros((self.del_num*(2*self.sz - self.del_num-1),2))\n",
    "            for i in range(out.size()[0]):\n",
    "                hh[i, out[i]] = 1\n",
    "            out = hh                    \n",
    "        out = out[:, 0]\n",
    "        if use_cuda:\n",
    "            out = out.cuda()\n",
    "            \n",
    "        matrix = torch.zeros(self.sz,self.sz).cuda()\n",
    "        left_mask = torch.ones(self.sz,self.sz)\n",
    "        left_mask[:-self.del_num,:-self.del_num] = 0\n",
    "        left_mask = left_mask - torch.diag(torch.diag(left_mask))\n",
    "        un_index = left_mask.nonzero()\n",
    "        matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "        out_matrix = matrix\n",
    "        # out_matrix = out[:, 0].view(self.gen_matrix.size()[0], self.gen_matrix.size()[0])\n",
    "        return out_matrix\n",
    "    def init(self, mean, var):\n",
    "        init.normal_(self.gen_matrix, mean=mean, std=var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0972e-01, 1.3107e-01, 1.3107e-01,  ..., 1.2500e-02, 1.2500e-02,\n",
      "         7.8125e-03],\n",
      "        [1.0486e-01, 2.6214e-02, 6.5536e-02,  ..., 2.5000e-03, 6.2500e-03,\n",
      "         1.5625e-03],\n",
      "        [1.0486e-01, 6.5536e-02, 2.6214e-02,  ..., 6.2500e-03, 2.5000e-03,\n",
      "         1.5625e-03],\n",
      "        ...,\n",
      "        [3.2768e-03, 8.1920e-04, 2.0480e-03,  ..., 8.0000e-07, 2.0000e-06,\n",
      "         5.0000e-07],\n",
      "        [3.2768e-03, 2.0480e-03, 8.1920e-04,  ..., 2.0000e-06, 8.0000e-07,\n",
      "         5.0000e-07],\n",
      "        [1.6384e-03, 4.0960e-04, 4.0960e-04,  ..., 4.0000e-07, 4.0000e-07,\n",
      "         1.0000e-07]], grad_fn=<ViewBackward>)\n",
      "tensor([[2.0972e-01, 1.3107e-01, 1.3107e-01,  ..., 1.2500e-02, 1.2500e-02,\n",
      "         7.8125e-03],\n",
      "        [1.0486e-01, 2.6214e-02, 6.5536e-02,  ..., 2.5000e-03, 6.2500e-03,\n",
      "         1.5625e-03],\n",
      "        [1.0486e-01, 6.5536e-02, 2.6214e-02,  ..., 6.2500e-03, 2.5000e-03,\n",
      "         1.5625e-03],\n",
      "        ...,\n",
      "        [3.2768e-03, 8.1920e-04, 2.0480e-03,  ..., 8.0000e-07, 2.0000e-06,\n",
      "         5.0000e-07],\n",
      "        [3.2768e-03, 2.0480e-03, 8.1920e-04,  ..., 2.0000e-06, 8.0000e-07,\n",
      "         5.0000e-07],\n",
      "        [1.6384e-03, 4.0960e-04, 4.0960e-04,  ..., 4.0000e-07, 4.0000e-07,\n",
      "         1.0000e-07]], grad_fn=<MmBackward>) tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.0972e-01, 1.3107e-01, 1.3107e-01,  ..., 1.2500e-02, 1.2500e-02,\n",
       "         7.8125e-03],\n",
       "        [1.0486e-01, 2.6214e-02, 6.5536e-02,  ..., 2.5000e-03, 6.2500e-03,\n",
       "         1.5625e-03],\n",
       "        [1.0486e-01, 6.5536e-02, 2.6214e-02,  ..., 6.2500e-03, 2.5000e-03,\n",
       "         1.5625e-03],\n",
       "        ...,\n",
       "        [3.2768e-03, 8.1920e-04, 2.0480e-03,  ..., 8.0000e-07, 2.0000e-06,\n",
       "         5.0000e-07],\n",
       "        [3.2768e-03, 2.0480e-03, 8.1920e-04,  ..., 2.0000e-06, 8.0000e-07,\n",
       "         5.0000e-07],\n",
       "        [1.6384e-03, 4.0960e-04, 4.0960e-04,  ..., 4.0000e-07, 4.0000e-07,\n",
       "         1.0000e-07]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk = generator.shuffle_adj(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_non_obs = torch.FloatTensor(label_non_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0972e-01, 1.3107e-01, 1.3107e-01,  ..., 1.2500e-02, 1.2500e-02,\n",
       "         7.8125e-03],\n",
       "        [1.0486e-01, 2.6214e-02, 6.5536e-02,  ..., 2.5000e-03, 6.2500e-03,\n",
       "         1.5625e-03],\n",
       "        [1.0486e-01, 6.5536e-02, 2.6214e-02,  ..., 6.2500e-03, 2.5000e-03,\n",
       "         1.5625e-03],\n",
       "        ...,\n",
       "        [3.2768e-03, 8.1920e-04, 2.0480e-03,  ..., 8.0000e-07, 2.0000e-06,\n",
       "         5.0000e-07],\n",
       "        [3.2768e-03, 2.0480e-03, 8.1920e-04,  ..., 2.0000e-06, 8.0000e-07,\n",
       "         5.0000e-07],\n",
       "        [1.6384e-03, 4.0960e-04, 4.0960e-04,  ..., 4.0000e-07, 4.0000e-07,\n",
       "         1.0000e-07]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3107e-01, 8.6893e-01],\n",
       "        [8.1920e-02, 9.1808e-01],\n",
       "        [5.1200e-02, 9.4880e-01],\n",
       "        ...,\n",
       "        [1.6000e-06, 1.0000e+00],\n",
       "        [1.6000e-06, 1.0000e+00],\n",
       "        [1.6000e-06, 1.0000e+00]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def sample_all(self,hard=False):\n",
    "'''sample from kron para'''\n",
    "self.logp = self.gen_matrix\n",
    "\n",
    "\n",
    "# out_matrix = out[:, 0].view(self.gen_matrix.size()[0], self.gen_matrix.size()[0])\n",
    "return out_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    self.logp = self.gen_matrix.cuda()\n",
    "out = gumbel_softmax(self.logp, self.temperature, hard)\n",
    "\n",
    "if hard:\n",
    "    hh = torch.zeros((self.del_num*(2*self.sz - self.del_num-1),2))\n",
    "    for i in range(out.size()[0]):\n",
    "        hh[i, out[i]] = 1\n",
    "    out = hh                    \n",
    "out = out[:, 0]\n",
    "if use_cuda:\n",
    "    out = out.cuda()\n",
    "matrix = torch.zeros(self.sz,self.sz).cuda()\n",
    "left_mask = torch.ones(self.sz,self.sz)\n",
    "left_mask[:-self.del_num,:-self.del_num] = 0\n",
    "left_mask = left_mask - torch.diag(torch.diag(left_mask))\n",
    "un_index = left_mask.nonzero()\n",
    "matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "out_matrix = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if hard:\n",
    "            hh = torch.zeros((self.del_num*(2*self.sz - self.del_num-1),2))\n",
    "            for i in range(out.size()[0]):\n",
    "                hh[i, out[i]] = 1\n",
    "            out = hh                    \n",
    "        out = out[:, 0]\n",
    "        if use_cuda:\n",
    "            out = out.cuda()\n",
    "        matrix = torch.zeros(self.sz,self.sz).cuda()\n",
    "        left_mask = torch.ones(self.sz,self.sz)\n",
    "        left_mask[:-self.del_num,:-self.del_num] = 0\n",
    "        left_mask = left_mask - torch.diag(torch.diag(left_mask))\n",
    "        un_index = left_mask.nonzero()\n",
    "        matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "        out_matrix = matrix\n",
    "        # out_matrix = out[:, 0].view(self.gen_matrix.size()[0], self.gen_matrix.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3107e-01],\n",
       "        [8.1920e-02],\n",
       "        [5.1200e-02],\n",
       "        ...,\n",
       "        [1.6000e-06],\n",
       "        [1.6000e-06],\n",
       "        [1.6000e-06]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_all \n",
    "\n",
    "\n",
    "for i in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SamplePermutationPk(Pk,pos_1,pos_2,index_pk,A,u):\n",
    "    Pk=Pk.ravel()[index_pk.ravel()].reshape(Pk.shape)\n",
    "    index_pk_later=SwapElement(index_pk,pos_1,pos_2)\n",
    "    print(pos_1,pos_2)\n",
    "#     print(index_pk_later[pos_1],index_pk_later[pos_2])\n",
    "    Pk_later=Pk.ravel()[index_pk_later.ravel()].reshape(Pk.shape)\n",
    "    print((Pk[pos_1]-Pk_later[pos_2]).sum())\n",
    "    ratio=metropolis_update_ratio_Pk(Pk,Pk_later,A)\n",
    "#     print(ratio)\n",
    "    if u<ratio:\n",
    "        Pk=Pk_later\n",
    "        index_pk=index_pk_later\n",
    "        print((Pk[pos_1]-Pk_later[pos_2]).sum())\n",
    "#         print(index_pk_later)\n",
    "    return index_pk,Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4231, 0.5536],\n",
       "        [0.8359, 0.3100],\n",
       "        [0.4056, 0.9016],\n",
       "        ...,\n",
       "        [0.5848, 0.5582],\n",
       "        [0.6054, 0.2039],\n",
       "        [0.4154, 0.2149]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(del_num*(2*sz-del_num-1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_index(Pk):\n",
    "    index=np.arange(len(Pk.ravel())).reshape(Pk.shape) \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ground_truth_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma=Ground_truth_adj\n",
    "baseline = loss_func(Ground_truth_adj_before,ground_adj,1)\n",
    "print(\"base\",baseline)\n",
    "\n",
    "np_baseline = NLL(Ground_truth_adj_before.data.numpy(),ground_adj.data.numpy())\n",
    "print(\"base2\",np_baseline)\n",
    "\n",
    "# shuffle \n",
    "node_num=sz\n",
    "permu_m = torch.eye(node_num)\n",
    "permutetion_arrange = torch.randperm(node_num)\n",
    "permu_m_shuffle= torch.index_select(permu_m,0,permutetion_arrange) \n",
    "\n",
    "# Ground_truth_adj = torch.mm(permu_m_shuffle,Ground_truth_adj_before).mm(permu_m_shuffle.t())\n",
    "Ground_truth_adj =Ground_truth_adj_before\n",
    "mask_un_obs,mask_obs,miss_idx = missing_label(sz,remove_proportion)\n",
    "G = Ground_truth_adj*mask_obs\n",
    "init_z = Ground_truth_adj*mask_un_obs\n",
    "missing_edges = int(init_z.sum())\n",
    "\n",
    "# initial partial z with fixed missing edges num\n",
    "z_ele_num = int(mask_un_obs.sum())\n",
    "z_element_choice = torch.randperm(z_ele_num)[:missing_edges]\n",
    "z_element = torch.nonzero(mask_un_obs)\n",
    "init_z_edges = torch.index_select(z_element,0,z_element_choice)\n",
    "H = G.clone()\n",
    "H[init_z_edges[:,0],init_z_edges[:,1]] = 1\n",
    "# print(G,H,z_element_choice)\n",
    "# initial kronecker\n",
    "\n",
    "p0 = torch.FloatTensor([[0.8, 0.5],[0.4, 0.1]])   # torch.rand([2,2])\n",
    "generator = kronecker_Generator(p0,k,2)\n",
    "Pk = generator.generator_adjacency()\n",
    "init_H = H.data.numpy()\n",
    "label_non_obs = mask_un_obs.data.numpy()\n",
    "init_Pk = Pk.detach().data.numpy()\n",
    "init_label_non_obs= label_non_obs.copy()\n",
    "objective_adj = np.array(Ground_truth_adj)\n",
    "\n",
    "calauc(Ground_truth_adj_before.numpy(),ground_adj.numpy(),(1-label_non_obs).astype(bool))\n",
    "c=(ground_adj*Ground_truth_adj_before)[label_non_obs.astype(bool)][(ground_adj*Ground_truth_adj_before)[label_non_obs.astype(bool)]>0]\n",
    "print(np.sort(c)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_address =\"/data/chenmy/voter/seed1051email128-adjmat.pickle\"\n",
    "with open(adj_address,'rb') as f:\n",
    "    objective_adj = pickle.load(f,encoding='latin1')\n",
    "objective_adj = np.array(objective_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss nodes  tensor([ 35, 111,  22,  27,  51, 126,  83,  69,  74,  91,  99,  25])\n",
      "miss_edge 2928\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Ground_truth_adj = torch.FloatTensor(objective_adj)\n",
    "sz = Ground_truth_adj.shape[0]\n",
    "k = int(np.log2(sz))\n",
    "remove_proportion = 0.1\n",
    "del_num = int(sz*remove_proportion)\n",
    "mask_un_obs,mask_obs,miss_idx = missing_label(sz,remove_proportion)\n",
    "G = Ground_truth_adj*mask_obs\n",
    "nG = G.data.numpy()\n",
    "H = G.clone()\n",
    "# print(2,(abs(H.data.numpy()-objective_adj)*mask_obs.data.numpy()).sum())\n",
    "init_z = Ground_truth_adj*mask_un_obs\n",
    "missing_edges = int(init_z.sum())\n",
    "print(\"miss nodes \",miss_idx)\n",
    "# initial partial z with fixed missing edges num\n",
    "z_ele_num = int(mask_un_obs.sum())\n",
    "print(\"miss_edge\",z_ele_num)\n",
    "z_element_choice = torch.randperm(z_ele_num)[:missing_edges]\n",
    "z_element = torch.nonzero(mask_un_obs)\n",
    "init_z_edges = torch.index_select(z_element,0,z_element_choice)\n",
    "H[init_z_edges[:,0],init_z_edges[:,1]] = 1\n",
    "# print(abs((H - Ground_truth_adj)*mask_obs).sum())\n",
    "# print(G,H,z_element_choice)\n",
    "# initial kronecker\n",
    "p0 = torch.FloatTensor([[0.9,0.7],[0.7, 0.3]])#torch.FloatTensor([[0.4408, 0.1770],[0.4951, 0.2585]])  # \n",
    "generator = kronecker_Generator(p0,k,2)\n",
    "Pk = generator.generator_adjacency()\n",
    "init_H = H.data.numpy()\n",
    "label_non_obs = mask_un_obs.data.numpy()\n",
    "init_Pk = Pk.detach().data.numpy()\n",
    "perm = np.eye(sz)\n",
    "print(abs((init_H- objective_adj)*mask_obs.data.numpy()).sum())\n",
    "# init_label_non_obs= label_non_obs.copy()\n",
    "init_H_ori = init_H.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(shape, eps=1e-20):\n",
    "    u = torch.rand(shape)\n",
    "    gumbel = - np.log(- np.log(u + eps) + eps)\n",
    "    if use_cuda:\n",
    "        gumbel = gumbel.cuda()\n",
    "    return gumbel\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    # gumbel_sample 返回一个sample采样\n",
    "    y = logits + gumbel_sample(logits.size())\n",
    "    return torch.nn.functional.softmax(y/temperature, dim=1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "    Args:\n",
    "    logits: [batch_size, n_class] unnormalized log-probs\n",
    "    temperature: non-negative scalar\n",
    "    hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "    Returns:\n",
    "    [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "    If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "    be a probabilitiy distribution that sums to 1 across classes\n",
    "    \"\"\"\n",
    "    \n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if hard:\n",
    "        k = logits.size()[-1]\n",
    "        y_hard = torch.max(y.data, 1)[1]\n",
    "        y = y_hard\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load kronEM.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import numpy as np\n",
    "\n",
    "def kronecker(A,B):\n",
    "    return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "class kronecker_Generator(nn.Module):\n",
    "    def __init__(self,p0,korder = 3,node_num = 2):\n",
    "        super(kronecker_Generator,self).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        # self.p = Parameter(torch.rand(node_num,node_num,requires_grad=True))\n",
    "        self.korder = korder\n",
    "        # print(self.p)\n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj\n",
    "def loss_func(sigma,Pk):\n",
    "    loss = -torch.sum((1-sigma)*torch.log(1-Pk)+sigma*torch.log(Pk))\n",
    "    return loss\n",
    "def metropolis_update_ratio(sigma_before,sigma_later,Pk):\n",
    "    '''\n",
    "    if memory is sufficinet, this one is much cleaner to execute\n",
    "    '''\n",
    "    Nll_before=(1-sigma_before)*np.log(1-Pk) +sigma_before*np.log(Pk)\n",
    "    Nll_later=(1-sigma_later)*np.log(1-Pk) +sigma_later*np.log(Pk)\n",
    "    ratio=np.exp(np.sum(Nll_later-Nll_before))\n",
    "    return ratio\n",
    "def SwapElement(sigma_before,i,j):\n",
    "    i_topology=sigma_before[i,:]\n",
    "    j_topology=sigma_before[j,:]\n",
    "    sigma_later=np.copy(sigma_before)\n",
    "    sigma_later[i,:]=j_topology\n",
    "    sigma_later[j,:]=i_topology\n",
    "    sigma_later[:,i]=sigma_before[:,j]\n",
    "    sigma_later[:,j]=sigma_before[:,i]\n",
    "    sigma_later[i,j]=sigma_before[i,j]\n",
    "    sigma_later[j,i]=sigma_before[j,i]\n",
    "    sigma_later[i,i]=sigma_before[j,j]\n",
    "    sigma_later[j,j]=sigma_before[i,i]\n",
    "    return sigma_later\n",
    "\n",
    "def SamplePermutation(Pk,sigma,u,n1_swap,n2_swap,label_non_obs,perm):\n",
    "    sigma_later=SwapElement(sigma,n1_swap, n2_swap)\n",
    "    ratio=metropolis_update_ratio(sigma,sigma_later,Pk)\n",
    "#     Pk_ori = Pk.copy()\n",
    "    if u<ratio:\n",
    "        sigma=sigma_later\n",
    "        label_non_obs=SwapElement(label_non_obs,n1_swap, n2_swap)\n",
    "        perm = SwapElement(perm,n1_swap,n2_swap)\n",
    "#         perm[n1_swap], perm[n2_swap] =  perm[n2_swap],perm[n1_swap]\n",
    "#         Pk_ori = SwapElement(Pk_ori,n1_swap,n2_swap)\n",
    "#     print(abs(Pk_ori-Pk).sum())\n",
    "    return sigma,label_non_obs,perm\n",
    "def SampleZ(H,Pk,label_non_obs,u):\n",
    "    mat_size=len(Pk)\n",
    "    edge_in_non_obs=(H>0)*label_non_obs  \n",
    "    edge_position=np.where(edge_in_non_obs)\n",
    "    edge_removed=np.random.randint(len(edge_position[0]))\n",
    "    px=Pk[edge_position[0][edge_removed],edge_position[1][edge_removed]]\n",
    "    non_edge_in_non_obs=(H<1)*label_non_obs\n",
    "    ##return \n",
    "    py_array=non_edge_in_non_obs*Pk\n",
    "    py_array=np.ravel(py_array)\n",
    "    py=np.random.choice(range(len(py_array)), size=1, p=py_array/np.sum(py_array))\n",
    "    ratio=(1-py_array[py])/(1-px)\n",
    "    if ratio<u:\n",
    "        H[py//mat_size,py%mat_size]=1\n",
    "        H[edge_position[0][edge_removed],edge_position[1][edge_removed]]=0\n",
    "        # print('accept')\n",
    "    return H\n",
    "\n",
    "def missing_label(sz,missing_percent):\n",
    "     # random sample\n",
    "    missing_num = int(sz*missing_percent)\n",
    "    idx = torch.randperm(sz)[:missing_num]\n",
    "    mask_un_obs = torch.zeros(sz,sz)\n",
    "    for i in idx:\n",
    "        mask_un_obs[i] = 1\n",
    "        mask_un_obs[:,i:i+1] = 1\n",
    "    mask_obs = 1- mask_un_obs\n",
    "    return mask_un_obs,mask_obs,idx\n",
    "# only sigma and kronecker pk are itered\n",
    "def E_step(sigma,N,warmup,Pk,label_non_obs,perm):\n",
    "    u1=np.random.rand(N+warmup)\n",
    "    u2=np.random.rand(N+warmup)\n",
    "    sigma_hist=[]\n",
    "    Z_label=[]\n",
    "    Node_list=np.arange(len(Pk))\n",
    "    element_to_swap=np.random.choice(a=Node_list,size=(2,3*(N+warmup)))\n",
    "    mask=element_to_swap[1,:]!=element_to_swap[0,:]# it is pointless to swap the same element\n",
    "    n1_swap=element_to_swap[0,:][mask]\n",
    "    n2_swap=element_to_swap[1,:][mask]\n",
    "    \n",
    "    for i in range(warmup):\n",
    "        sigma,label_non_obs,Pk_ori=SamplePermutation(Pk,sigma,u2[i],n1_swap[i],n2_swap[i],label_non_obs,perm)  \n",
    "    for j in range(warmup):\n",
    "        sigma=SampleZ(sigma,Pk,label_non_obs,u1[i]) \n",
    "    for k in range(N):\n",
    "        sigma=SampleZ(sigma,Pk,label_non_obs,u1[i]) \n",
    "        sigma_hist.append(sigma)\n",
    "    \n",
    "    Z_label.append(label_non_obs)\n",
    "    return sigma_hist,Z_label,Pk_ori #（H+G）\n",
    "def M_step(epoch,sigma_train,p0,k,N):\n",
    "    losses = []\n",
    "    generator = kronecker_Generator(p0,k,2)\n",
    "    learning_rate = 1e-5#0.0000001\n",
    "    opt_net = optim.SGD(generator.parameters(),lr = learning_rate)\n",
    "    decayRate = 0.95\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt_net, gamma=decayRate) \n",
    "    for i in range(epoch):   \n",
    "        opt_net.zero_grad()\n",
    "        Pk = generator.generator_adjacency()\n",
    "        loss = loss_func(sigma_train,Pk,N)\n",
    "        loss2 = loss2_func(sigma_train.detach().numpy(),Pk.detach().numpy(),N)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        # print(str(i),loss.item(),loss2.item(),(loss2-loss).item())\n",
    "        opt_net.step()\n",
    "        scheduler.step()\n",
    "        for group in opt_net.param_groups:\n",
    "            for param in group[\"params\"]: \n",
    "              # print(\"before param\",param)\n",
    "              param.data.clamp_(0.0001,0.9999)\n",
    "              # print(\"after param\",param)\n",
    "    \n",
    "        for p in generator.parameters():\n",
    "            p0 = p.data  \n",
    "        generator = kronecker_Generator(p0,k,2)\n",
    "        Pk = generator.generator_adjacency()\n",
    "        Pk = Pk.detach().numpy() \n",
    "        # evaluation\n",
    "        #   nll infer  nll true\n",
    "    return np.mean(losses),Pk,p0\n",
    "\n",
    "def kronEM(iterstep,N,warmup,H,Pk,label_non_obs,epoch,p0,perm,obj_adj,k):\n",
    "    emlosses = []\n",
    "    for i in range(iterstep):\n",
    "        print(\"start iterstep\",i)\n",
    "        sigma_hist,Z_label,Pk_ori = E_step(H,N,warmup,Pk,label_non_obs,perm)\n",
    "        print(\"E-step\")\n",
    "        H = sigma_hist[-1]\n",
    "        label_non_obs = Z_label[-1]\n",
    "        sigma_hist_train = np.array(sigma_hist)\n",
    "        sigma_train = torch.DoubleTensor(sigma_hist_train)\n",
    "        emloss,Pk,p0 = M_step(epoch,sigma_train,p0,k,N)\n",
    "        emlosses.append(emloss)\n",
    "        print(\"*************\\n EM loss is %f\"%emloss,\"p0 is \",p0)\n",
    "        \n",
    "        # evaluation\n",
    "        inferp_nll = NLL(H,Pk)\n",
    "#         inferp_nll2 = NLL(obj_adj,Pk_ori)\n",
    "        \n",
    "        print(\"inferp_nll is %f \"%(inferp_nll))\n",
    "        obs_mask = (1-label_non_obs).astype(bool)\n",
    "#         print(\"2\",perm)\n",
    "#         perm_mat = np.eye(H.shape[0])[:,perm]\n",
    "        obj_adj = np.dot(perm,np.dot(obj_adj,perm.T))\n",
    "    \n",
    "#         obs_auc = calauc(obj_adj,Pk,obs_mask)\n",
    "#         non_obs_auc = calauc(obj_adj,Pk,label_non_obs.astype(bool))\n",
    "#         #         print(\"perm is \", perm)\n",
    "        \n",
    "        label_obs = (1-label_non_obs)\n",
    "        obs_diff_edge = abs((H - obj_adj)*label_obs).sum()\n",
    "        unobs_diff_edge =  abs((H - obj_adj)*label_non_obs).sum()\n",
    "        all_diff_edge = abs((H - obj_adj)).sum()\n",
    "        # D_abs = torch.abs(orip-p0).mean()\n",
    "#         \"obseved auc  is %f and non_obs auc is %f obs_auc,non_obs_auc,\n",
    "        print(\"infer p is \",p0, \"and obs_diff_edge is %f and un_obs_diff_edge %f and all_diff_edge is %f\"%(obs_diff_edge,unobs_diff_edge,all_diff_edge))\n",
    "    \n",
    "    return emlosses,perm,H,Pk\n",
    "\n",
    "def NLL(sigma_true,pk):\n",
    "    Nll_before=(1-sigma_true)*np.log(1-pk) +sigma_true*np.log(pk)\n",
    "    return -np.sum(Nll_before)\n",
    "\n",
    "def loss2_func(sigma_train,Pk,N):\n",
    "    loss2 = 0\n",
    "    for i in range(N):\n",
    "        loss2+= NLL(sigma_train[i],Pk)\n",
    "    # print(loss2)\n",
    "    return loss2/N\n",
    "\n",
    "def loss_func(sigma,Pk,N):\n",
    "    loss = -torch.sum((1-sigma)*torch.log(1-Pk)+sigma*torch.log(Pk))\n",
    "    return loss/N\n",
    "\n",
    "def generator_adj(korder,p):\n",
    "    k = korder\n",
    "    p0 = p\n",
    "    adj = p\n",
    "    for i in range(k-1):\n",
    "        adj = kronecker(adj,p0)\n",
    "    return adj\n",
    "def calauc(H,Pk,mask):\n",
    "    fpr, tpr, thresholds = roc_curve(H[mask],Pk[mask])\n",
    "    Auc = auc(fpr, tpr)\n",
    "    return Auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start iterstep 0\n",
      "E-step\n",
      "*************\n",
      " EM loss is 4129.971266 p0 is  tensor([[0.8963, 0.7064],\n",
      "        [0.7060, 0.3557]])\n",
      "inferp_nll is 4075.004395 \n",
      "infer p is  tensor([[0.8963, 0.7064],\n",
      "        [0.7060, 0.3557]]) and obs_diff_edge is 1230.000000 and un_obs_diff_edge 262.000000 and all_diff_edge is 1492.000000\n"
     ]
    }
   ],
   "source": [
    "warmup=100\n",
    "N = 1\n",
    "iterstep = 1\n",
    "epoch = 1\n",
    "emlosses2,perm2,H,pk= kronEM(iterstep,N,warmup,init_H,init_Pk,label_non_obs,epoch,p0,perm,init_H_ori,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_adj.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_para =  label_non_obs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kronecker(A,B):\n",
    "    return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "class kronecker_Generator(nn.Module):\n",
    "    def __init__(self,p0,korder = 3,node_num = 2):\n",
    "        super(kronecker_Generator,self).__init__()\n",
    "        self.p = Parameter(p0,requires_grad = True)\n",
    "        # self.p = Parameter(torch.rand(node_num,node_num,requires_grad=True))\n",
    "        self.korder = korder\n",
    "        # print(self.p)\n",
    "    def generator_adjacency(self):\n",
    "        k = self.korder\n",
    "        p0 = self.p\n",
    "        adj = self.p\n",
    "        for i in range(k-1):\n",
    "            adj = kronecker(adj,p0)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    u1=np.random.rand(N+warmup)\n",
    "    u2=np.random.rand(N+warmup)\n",
    "    sigma_hist=[]\n",
    "    Z_label=[]\n",
    "    Node_list=np.arange(len(Pk))\n",
    "    element_to_swap=np.random.choice(a=Node_list,size=(2,3*(N+warmup)))\n",
    "    mask=element_to_swap[1,:]!=element_to_swap[0,:]# it is pointless to swap the same element\n",
    "    n1_swap=element_to_swap[0,:][mask]\n",
    "    n2_swap=element_to_swap[1,:][mask]\n",
    "    \n",
    "    for i in range(warmup):\n",
    "        sigma,label_non_obs,obj_adj=SamplePermutation(Pk,sigma,u2[i],n1_swap[i],n2_swap[i],label_non_obs,obj_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''在网络补全任务中生成未知部分结构'''\n",
    "class Gumbel_Generator_nc_asy(nn.Module):\n",
    "    def __init__(self, sz=10,del_num = 1,temp=10, temp_drop_frac=0.9999):\n",
    "        super(Gumbel_Generator_nc_asy, self).__init__()\n",
    "        self.sz = sz\n",
    "        self.del_num = del_num\n",
    "        self.gen_matrix = Parameter(torch.rand(del_num*(2*sz-del_num-1), 2)) #cmy get only unknown part parameter\n",
    "        self.temperature = temp\n",
    "        self.temp_drop_frac = temp_drop_frac\n",
    "\n",
    "    def drop_temp(self):\n",
    "        # 降温过程\n",
    "        self.temperature = self.temperature * self.temp_drop_frac\n",
    "\n",
    "    def sample_all(self, hard=False):\n",
    "        self.logp = self.gen_matrix\n",
    "        if use_cuda:\n",
    "            self.logp = self.gen_matrix.cuda()\n",
    "        \n",
    "        out = gumbel_softmax(self.logp, self.temperature, hard)\n",
    "        if hard:\n",
    "            hh = torch.zeros((self.del_num*(2*self.sz - self.del_num-1),2))\n",
    "            for i in range(out.size()[0]):\n",
    "                hh[i, out[i]] = 1\n",
    "            out = hh                    \n",
    "        out = out[:, 0]\n",
    "        if use_cuda:\n",
    "            out = out.cuda()\n",
    "            \n",
    "        matrix = torch.zeros(self.sz,self.sz).cuda()\n",
    "        left_mask = torch.ones(self.sz,self.sz)\n",
    "        left_mask[:-self.del_num,:-self.del_num] = 0\n",
    "        left_mask = left_mask - torch.diag(torch.diag(left_mask))\n",
    "        un_index = left_mask.nonzero()\n",
    "        matrix[(un_index[:,0],un_index[:,1])] = out\n",
    "        out_matrix = matrix\n",
    "        # out_matrix = out[:, 0].view(self.gen_matrix.size()[0], self.gen_matrix.size()[0])\n",
    "        return out_matrix\n",
    "    def init(self, mean, var):\n",
    "        init.normal_(self.gen_matrix, mean=mean, std=var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## union code \n",
    "kronfit 已知部分 + gumbel   + 两个loss （ll+states loss）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  85,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  65,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        37,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  22,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_mat  = np.eye(H.shape[0])[perm]\n",
    "shuffle_H = np.dot(np.dot(permutation_mat,objective_adj),permutation_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_later1=SwapElement(init_H,37,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_later=SwapElement(sigma_later,22,85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(sigma_later1-H)*mask_obs.data.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(shuffle_H-H)*mask_obs.data.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch03",
   "language": "python",
   "name": "torch03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
